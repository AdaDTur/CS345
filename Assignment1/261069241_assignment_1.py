# -*- coding: utf-8 -*-
"""261069241-Assignment-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/144Zdag-862C2hI_gw_mrwgxufxkjYo-l

Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this assignment to your Google drive.

<img src="https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS" alt="Drawing" height="300"/>

Caution: Since this is real language on Twitter and deals with world events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. 

First, let's read the tweets from the Google drive and print a few lines from the file.

Run the below code snippet. It will generate a URL which generates an authorization code. Enter it below to give Colab access to your Google drive. 

**Note:** Copy button may not work. Try copying the authorization code manually.
"""

from google.colab import drive
drive.mount('/content/drive/')

"""When you run the `ls` command below, you should see the files in the tweets folder.



"""

!ls "/content/drive/My Drive/tweets"

"""Let's read the top 10 tweets from the file `20000_tweets.txt` and print them."""

f = open("/content/drive/My Drive/tweets/20000_tweets.txt", "r")

line_count = 0
top_tweets = []
for tweet in f:
  print("### Tweet", line_count, "#####")
  print(tweet)
  
  top_tweets.append(tweet)
  line_count += 1
  if line_count >= 10:
    break
f.close()

"""# **Problem 1: Word Tokenizer**

Implement your own tokenizer for tweets without using libraries like `nltk`, `spacy` or any existing tokenizers. Tokenizer is a fundamental tool for processing any text data. The tokenizer should preserve punctuations.

The outputs for the top five tweets should approximately be as follows (it's fine to not exactly match this output)

```
['Covid-19', 'Economic', 'Response', ':', 'Cancel', 'Student', 'Loans', 'by', 'Executive', 'Order', '.', '-', 'Sign', 'the', 'Petition', '!', 'https://t.co/BnPXWHv5cr', 'via', '@Change']
['Hey', '!', 'The', 'stock', 'markets', 'up', '!', 'Fuck', 'Trump', 'and', 'the', 'Trumpublican', 'Senate', '!', 'https://t.co/4t6mgbaG2C']
['@ProjectLincoln', 'blame', 'Trump', 'for', 'no', 'sports', 'because', 'of', 'Covid19', '@MeidasTouch']
['Do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'Lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', '?', 'Nothing', 'is', 'going', 'to', 'change', 'I', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', ',', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'I', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']
['@NBCSAthletics', 'Ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', '...', 'Covid', 'or', 'not', '.', 'Behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', ',', 'sadly']
```

A thing to note is that words like `wouldn't` and `you'll` become two tokens `would n't` and `you 'll`, after tokenization. Treating `'nt` and  `'ll` as independent tokens is common in NLP tools. You can come up with that kind of list easily, e.g., `'s, 've`.

"""

# import any relevant libraries here
import re
# a function to tokenize text into words
def tokenize(text):
  text = re.sub('([A-Za-z]+)\s?[\'’\!]\s?([A-Za-z]+)', r'\1'' ’'r'\2', text)
  text = re.sub('([!.:?-])+\s', ' 'r'\1'' ', text)
  text = re.sub('wouldn\s’t', 'would n’t', text)
  words = text.split()  
  return list(filter(None, words))

tokenized_top_tweets = [tokenize(tweet) for tweet in top_tweets]
for tokenized_tweet in tokenized_top_tweets:
  print(tokenized_tweet)

"""# Problem 2: Post-process the tokenized tweets

Clean the tokenized tweets such that usernames are repalced with `@USER`, URLs are replaced with `URL`, punctuations are removed and words are lower-cased.

The output for the top ten tweets after cleaning should look something like this

```
['covid-19', 'economic', 'response', 'cancel', 'student', 'loans', 'by', 'executive', 'order', 'sign', 'the', 'petition', 'URL', 'via', '@USER']
['hey', 'the', 'stock', 'markets', 'up', 'fuck', 'trump', 'and', 'the', 'trumpublican', 'senate', 'URL']
['@USER', 'blame', 'trump', 'for', 'no', 'sports', 'because', 'of', 'covid19', '@USER']
['do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', 'nothing', 'is', 'going', 'to', 'change', 'i', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'i', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']
['@USER', 'ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', 'covid', 'or', 'not', 'behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', 'sadly']
```
"""

# import relevant packages

# function to clean a tweet
def clean_a_tweet(tokenized_tweet):
  tokenized_tweet = list(filter(None, tokenized_tweet))
  for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = tokenized_tweet[i].lower()
    tokenized_tweet[i] = re.sub('([!.:?-])', '', tokenized_tweet[i])
    tokenized_tweet[i] = tokenized_tweet[i].replace('covid19', 'covid-19')
    if "http" in tokenized_tweet[i]:
      tokenized_tweet[i] = "URL"
    if "@" in tokenized_tweet[i]:
      tokenized_tweet[i] = "@USER"
    

  clean_tokenized_tweet = list(filter(None, tokenized_tweet))
  return(clean_tokenized_tweet)

anonymized_top_tweets = [clean_a_tweet(tokenized_tweet) for tokenized_tweet in tokenized_top_tweets]
for tokenized_tweet in anonymized_top_tweets:
  print(tokenized_tweet)

"""# Problem 3: Unigrams and Bigrams

Read **all** tweets from the file `20000_tweets.txt` and print the top 10 unigrams and bigrams. Consider only the English tweets. You have to use `tokenize` and `clean_a_tweet` function.

Output format

```
Top 10 Unigrams
@USER	16698
the	16666
URL	12610
to	11866
a	9845
....
....
....
....
....

Top 10 Bigrams
@USER @USER	7494
in the	1280
of the	1265
it ’s	860
a mask	793
....
....
....
....
....
```

Your numbers need not match the above word frequencies. Depending on your tokenizer, you may get different numbers
"""

# Write your code to build top 10 unigrams and bigrams
# Hint: You can use python dictionary or collections.Counter().
file = open("/content/drive/My Drive/tweets/20000_tweets.txt", "r")

unigrams = {}
bigrams = {}

for tweet in file:
  tokenized = tokenize(tweet)
  cleaned = clean_a_tweet(tokenized)
  for i in range(len(cleaned)):
    if cleaned[i] not in unigrams:
      unigrams[cleaned[i]] = 1
    else:
      unigrams[cleaned[i]] += 1

  for i in range(len(cleaned) - 1):
    if (cleaned[i] + ' ' + cleaned[i + 1]) not in bigrams:
      bigrams[cleaned[i] + ' ' +  cleaned[i + 1]] = 1
    else:
      bigrams[cleaned[i] + ' ' +  cleaned[i + 1]] += 1

unigramsList = sorted(unigrams.items(), key=lambda kv: kv[1], reverse=True)
bigramsList = sorted(bigrams.items(), key=lambda kv: kv[1], reverse=True)
print("Top 10 Unigrams")
for i in range(10):
  print(str(unigramsList[i][0]) + '\t' + str(unigramsList[i][1]))
print("\n \nTop 10 Bigrams")
for i in range(10):
  print(str(bigramsList[i][0]) + '\t' + str(bigramsList[i][1]))

#turning back into dict
unigramsDict = {}
bigramsDict = {}
for i in range(len(unigramsList)):
  unigramsDict[unigramsList[i][0]] = bigramsList[i][1]
for i in range(len(bigramsList)):
  bigramsDict[bigramsList[i][0]] = bigramsList[i][1]

"""# Problem 4: How frequent are the following unigrams and bigrams?

```
covid
coronavirus
republicans
democrats
social distancing
wear mask
no mask
donald trump
joe biden
```

Output:
```
covid	4437
coronavirus	1368
...
...
social distancing	568
wear mask	19
...
...
...
```

"""

# Write your code to print the frequencies of the above unigrams and bigrams
# Hint: reuse the dictionaries you built in the previous cell
uniList = ["covid", "coronavirus", "republicans", "democrats"]
biList = ["social distancing", "wear mask", "no mask", "donald trump", "joe biden"]
outputList = [[]]
for word in uniList:
  print(word + '\t' + str(unigramsDict.get(word)))
for word in biList:
  print(word + '\t' + str(bigramsDict.get(word)))